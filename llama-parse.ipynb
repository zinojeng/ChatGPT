{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9esgcwaz2Nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "a341cae0-149a-4989-989d-0bbd25d98ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.25.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n"
          ]
        }
      ],
      "source": [
        "# @title 安裝 Llama-index 、 Llama-parse 與 OpenAI\n",
        "!pip install llama-index\n",
        "!pip install llama-index-core\n",
        "!pip install llama-index-embeddings-openai\n",
        "!pip install llama-parse\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 存取 Google Drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "THESIS_LOC = '/content/drive/MyDrive/thesis/' # @param {type:\"string\"}\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(THESIS_LOC)\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "jL6mypYcZxw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "7cc7d34a-4143-4044-8f93-3579b8295ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2404.17723.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-1_9yYkz2N0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title 設定 llama-parse\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import os\n",
        "LLAMA_PARSER_KEY = 'llx-'  # @param {type:\"string\"}\n",
        "OPENAI_KEY = 'sk-'  # @param {type:\"string\"}\n",
        "os.environ[\"LLAMA_CLOUD_API_KEY\"] = LLAMA_PARSER_KEY\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcd_LcPYz2N0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title 設定llama-parse 用的 LLM 模型\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core import Settings\n",
        "LLM_MODEL = \"gpt-3.5-turbo-0125\"  # @param {type:\"string\"}\n",
        "llm = OpenAI(model=LLM_MODEL)\n",
        "\n",
        "Settings.llm = llm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 載入PDF\n",
        "from llama_parse import LlamaParse\n",
        "from IPython.display import display, Markdown, Latex\n",
        "import os\n",
        "\n",
        "PDF_FILE = \"2404.17723.pdf\" # @param {type:\"string\"}\n",
        "parser = LlamaParse(result_type=\"markdown\")\n",
        "\n",
        "md_documents = parser.load_data(\n",
        "    file_path=PDF_FILE\n",
        ")\n",
        "\n",
        "print(md_documents[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4cMWhCfr7JA",
        "outputId": "adbc99a7-9d62-4c69-9ea7-4bb17c0102a4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started parsing the file under job_id df1addcd-818f-45aa-88d2-d3c7a8082221\n",
            "## Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering\n",
            "\n",
            "Zhentao Xu &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Mark Jerome Cruz &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Matthew Guevara\n",
            "\n",
            "zhexu@linkedin.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; marcruz@linkedin.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mguevara@linkedin.com\n",
            "\n",
            "LinkedIn Corporation &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; LinkedIn Corporation &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; LinkedIn Corporation\n",
            "\n",
            "Sunnyvale, CA, USA &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Sunnyvale, CA, USA &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Sunnyvale, CA, USA\n",
            "\n",
            "Tie Wang &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Manasi Deshpande &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Xiaofeng Wang\n",
            "\n",
            "tiewang@linkedin.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; madeshpande@linkedin.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; xiaofwang@linkedin.com\n",
            "\n",
            "LinkedIn Corporation &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; LinkedIn Corporation &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; LinkedIn Corporation\n",
            "\n",
            "Sunnyvale, CA, USA &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Sunnyvale, CA, USA &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Sunnyvale, CA, USA\n",
            "\n",
            "Zheng Li\n",
            "\n",
            "zeli@linkedin.com\n",
            "\n",
            "LinkedIn Corporation\n",
            "\n",
            "Sunnyvale, CA, USA\n",
            "\n",
            "### ABSTRACT\n",
            "\n",
            "In customer service technical support, swiftly and accurately retrieving relevant past issues is critical for efficiently resolving customer inquiries. The conventional retrieval methods in retrieval-augmented generation (RAG) for large language models (LLMs) treat a large corpus of past issue tracking tickets as plain text, ignoring the crucial intra-issue structure and inter-issue relations, which limits performance. We introduce a novel customer service question-answering method that amalgamates RAG with a knowledge graph (KG). Our method constructs a KG from historical issues for use in retrieval, retaining the intra-issue structure and inter-issue relations. During the question-answering phase, our method parses consumer queries and retrieves related sub-graphs from the KG to generate answers. This integration of a KG not only improves retrieval accuracy by preserving customer service structure information but also enhances answering quality by mitigating the effects of text segmentation. Empirical assessments on our benchmark datasets, utilizing key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR) metrics, reveal that our method outperforms the baseline by 77.6% in MRR and by 0.32 in BLEU. Our method has been deployed within LinkedIn’s customer service team for approximately six months and has reduced the median per-issue resolution time by 28.6%.\n",
            "\n",
            "### CCS CONCEPTS\n",
            "\n",
            "• Computing methodologies → Information extraction; Natural language generation.\n",
            "\n",
            "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n",
            "\n",
            "SIGIR ’24, July 14–18, 2024, Washington, DC, USA\n",
            "\n",
            "© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n",
            "\n",
            "ACM ISBN 979-8-4007-0431-4/24/07\n",
            "\n",
            "https://doi.org/10.1145/3626772.3661370\n",
            "\n",
            "### ACM Reference Format:\n",
            "\n",
            "Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, and Zheng Li. 2024. Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’24), July 14–18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 5 pages.\n",
            "\n",
            "https://doi.org/10.1145/3626772.3661370\n",
            "\n",
            "## 1 INTRODUCTION\n",
            "\n",
            "Effective technical support in customer service underpins product success, directly influencing customer satisfaction and loyalty. Given the frequent similarity of customer inquiries to previously resolved issues, the rapid and accurate retrieval of relevant past instances is crucial for the efficient resolution of such inquiries. Recent advancements in embedding-based retrieval (EBR), large language models (LLMs), and retrieval-augmented generation (RAG) have significantly enhanced retrieval performance and question-answering capabilities for the technical support of customer service. This process typically unfolds in two stages: first, historical issue tickets are treated as plain text, segmented into smaller chunks to accommodate the context length constraints of embedding models; each chunk is then converted into an embedding vector for retrieval. Second, during the question-answering phase, the system retrieves the most relevant chunks and feeds them as contexts for LLMs to generate answers in response to queries. Despite its straightforward approach, this method encounters several limitations:\n",
            "\n",
            "- Limitation 1 - Compromised Retrieval Accuracy from Ignoring Structures: Issue tracking documents such as Jira possess inherent structure and are interconnected, with references such as \"issue A is related to/copied from/caused\"\n",
            "---\n",
            "## SIGIR ’24, July 14–18, 2024, Washington, DC, USA Zhentao Xu, et al.\n",
            "\n",
            "Limitation 1 - Loss of Intrinsic Relationship: The conventional approach of compressing documents into text chunks leads to the loss of vital information. Our approach parses issue tickets into trees and further connects individual issue tickets to form an interconnected graph, which maintains this intrinsic relationship among entities, achieving high retrieval performance.\n",
            "\n",
            "Limitation 2 - Reduced Answer Quality from Segmentation: Segmenting extensive issue tickets into fixed-length segments to accommodate the context length constraints of embedding models can result in the disconnection of related content, leading to incomplete answers. Our graph-based parsing method overcomes this by preserving the logical coherence of ticket sections, ensuring the delivery of complete and high-quality responses.\n",
            "\n",
            "## RELATED WORK\n",
            "\n",
            "Question answering (QA) with knowledge graphs (KGs) can be broadly classified into retrieval-based, template-based, and semantic parsing-based methods. Retrieval-based approaches utilize relation extraction or distributed representations to derive answers from KGs, but they face difficulties with questions involving multiple entities. Template-based strategies depend on manually-created templates for encoding complex queries, yet are limited by the scope of available templates. Semantic parsing methods map text to logical forms containing predicates from KGs.\n",
            "\n",
            "Recent advancements in large language models (LLMs) integration with Knowledge Graphs (KGs) have demonstrated notable progress. For graph-based reasoning, Think-on-Graph and Reasoning-on-Graph enhance LLMs’ reasoning abilities by integrating KGs. LLM-based question answering systems employ KGs to boost inference capabilities in specialized domains.\n",
            "\n",
            "## METHODS\n",
            "\n",
            "We introduce an LLM-based customer service question answering system that seamlessly integrates retrieval-augmented generation (RAG) with a knowledge graph (KG). Our system comprises two phases: KG construction phase and question-answering phase. During the KG construction phase, our system constructs a comprehensive knowledge graph from historical customer service issue tickets. It integrates a tree-structured representation of each issue and interlinks them based on relational context. It also generates embedding for each node to facilitate later semantic searching. During the question-answering phase, our method parses consumer queries to identify named entities and intents, then navigates within the KG to identify related sub-graphs for generating answers.\n",
            "---\n",
            "## Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering SIGIR ’24, July 14–18, 2024, Washington, DC, USA\n",
            "\n",
            "|Knowledge Graph Construction|Retrieval and Question Answering|\n",
            "|---|---|\n",
            "|Ticket ENT-1744|Ticket ENT-3547|Ticket PORT-133061|CLONE_FROM|Ticket ENT-22970|Question Query: How to reproduce the issue where user saw \"csv upload error in updating user email\" and has major priority that was caused by data issue?|\n",
            "|Entity Detection|Intent Classification|\n",
            "|2 inter-ticket connection|1 intra-ticket connection|\n",
            "|Summary: \"CSV upload error in updating user email\"|Priority: Major|Root Cause: Data Issue|Intent: Steps to Reproduce|\n",
            "| |Embedding-based Retrieval|Filtering|Filtering|Question Intent|\n",
            "|Inter-ticket connections|Graph Database intra-ticket tree representation|\n",
            "|Ticket PORT-133061 \"CSV upload error, updating user email\"|CLONE_FROM|Ticket ENT-22970 \"CSV upload error, updating user email\"|\n",
            "|...|...|...|SIMILAR_TO|HAS_FIELDS|HAS_COMMENTS|\n",
            "|Ticket ENT-1744|HAS_SUMMARY|HTTP POST csv upload error-internal error|...|\n",
            "|Ticket ENT-3547|Learning 'upload csv' option fails|...|\n",
            "|Text-embedding Generation for Node Values|Final Answer: based on the ticket ENT-22970, the steps to reproduce the issue is \"1. Refer to the CSV: https://microsoft.sharepoint.com/xxx 2. Open the Dashboard ID xxxxxxxxx 3. Click on Instances > Profile 4. Search for users from the CSV file and note that there are 2 profiles exist.\"|\n",
            "\n",
            "Legends: issue-tracking ticket, step with step numbers, Vector DB, graph nodes with links, Step with LLM, Graph DB\n",
            "\n",
            "Figure 1: An overview of our proposed retrieval-augmented generation with knowledge graph framework. The left side of this diagram illustrates the knowledge graph construction; the right side shows the retrieval and question answering process.\n",
            "\n",
            "### 3.2 Retrieval and Question Answering\n",
            "\n",
            "#### 3.2.1 Query Entity Identification and Intent Detection.\n",
            "\n",
            "In this step, we extract the named entities P of type Map(N → V) and the query intent set I from each user query 𝑞. The method involves parsing each query 𝑞 into a key-value pair, where each key 𝑛, mentioned within the query, corresponds to an element in the graph template T, and the value 𝑣 represents the information extracted from the query. Concurrently, the query intents I include the entities mentioned in the graph template T that the query aims to address. We leverage LLM with a suitable prompt in this parsing process. For instance, given the query 𝑞 = \"How to reproduce the login issue where a user can’t log in to LinkedIn?\", the extracted entity is P = Map(\"issue summary\" → \"login issue\", \"issue description\" → \"user can’t log in to LinkedIn\"), and the intent set is I=Set(\"fix solution\"). This method demonstrates notable flexibility in accommodating varied query formulations by leveraging the LLM’s extensive understanding and interpretive capabilities.\n",
            "\n",
            "#### 3.2.2 Embedding-based Retrieval of Sub-graphs.\n",
            "\n",
            "Our method extracts pertinent sub-graphs from the knowledge graph, aligned with user-provided specifics such as \"issue description\" and \"issue summary\", as well as user intentions like \"fix solution\". This process consists of two primary steps: EBR-based ticket identification and LLM-driven subgraph extraction.\n",
            "---\n",
            "## SIGIR ’24, July 14–18, 2024, Washington, DC, USA\n",
            "\n",
            "Zhentao Xu, et al.\n",
            "\n",
            "the same ticket, we rank and select the top 𝐾 ticket tickets. This Table 1: Retrieval Performance method presupposes that the occurrence of multiple query entities is indicative of pertinent links, thus improving retrieval precision.\n",
            "\n",
            "| |MRR|Recall@K|NDCG@K|\n",
            "|---|---|---|---|\n",
            "|K=1|K=3|K=1|K=3|\n",
            "| |Baseline|0.522|0.400|0.640|0.400|0.520|\n",
            "| |Experiment|0.927|0.860|1.000|0.860|0.946|\n",
            "\n",
            "In the LLM-driven subgraph extraction step, the system first rephrases the original user query 𝑞 to include the retrieved ticket ID; the modified query 𝑞 ′ is then translated into a graph database language, such as Cypher for Neo4j for question answering.\n",
            "\n",
            "| |BLEU|METEOR|ROUGE|\n",
            "|---|---|---|---|\n",
            "|Baseline|0.057|0.279|0.183|\n",
            "|Experiment|0.377|0.613|0.546|\n",
            "\n",
            "### PRODUCTION USE CASE\n",
            "\n",
            "We deployed our method within LinkedIn’s customer service team, covering multiple product lines. The team was split randomly into two groups: one used our system, while the other stuck to traditional manual methods. As shown in Table 3, the group using our system achieved significant gains, reducing the median resolution time per issue by 28.6%. This highlights our system’s effectiveness in enhancing customer service efficiency.\n",
            "\n",
            "|Group|Mean|P50|P90|\n",
            "|---|---|---|---|\n",
            "|Tool Not Used|40 Hours|7 Hours|87 Hours|\n",
            "|Tool Used|15 hours|5 hours|47 hours|\n",
            "\n",
            "Answer Generation. Answers are synthesized by correlating retrieved data from Section 3.2.2 with the initial query. The LLM serves as a decoder to formulate responses to user inquiries given the retrieved information. For robust online serving, if query execution encounters issues, a fallback mechanism reverts to a baseline text-based retrieval method.\n",
            "\n",
            "### EXPERIMENT\n",
            "\n",
            "#### Experiment Design\n",
            "\n",
            "Our evaluation employed a curated \"golden\" dataset comprising typical queries, support tickets, and their authoritative solutions. The control group operated with conventional text-based EBR, while the experimental group applied the methodology outlined in this study. For both groups, we utilized the same LLM, specifically GPT-4 [ 1], and the same embedding model, E5 [17 ]. We measured retrieval efficacy using Mean Reciprocal Rank (MRR), recall@K, and NDCG@K. MRR gauges the average inverse rank of the initial correct response, recall@K determines the likelihood of a relevant item’s appearance within the top K selections, and NDCG@K appraises the rank quality by considering both position and pertinence of items. For question-answering performance, we juxtaposed the \"golden\" solutions against the generated responses, utilizing metrics such as BLEU [11], ROUGE [9], and METEOR [3] scores.\n",
            "\n",
            "#### Result and Analysis\n",
            "\n",
            "The retrieval and question-answering performances are presented in Table 1 and Table 2, respectively. Across all metrics, our method demonstrates consistent improvements. Notably, it surpasses the baseline by 77.6% in MRR and by 0.32 in BLEU score, substantiating its superior retrieval efficacy and question-answering accuracy.\n",
            "\n",
            "### CONCLUSIONS AND FUTURE WORK\n",
            "\n",
            "In conclusion, our research significantly advances automated question answering systems for customer service. Integrating retrieval augmented generation (RAG) with a knowledge graph (KG) has improved retrieval and answering metrics, and overall service effectiveness. Future work will focus on: developing an automated mechanism for extracting graph templates, enhancing system adaptability; investigating dynamic updates to the knowledge graph based on user queries to improve real-time responsiveness; and exploring the system’s applicability in other contexts beyond customer service.\n",
            "\n",
            "### COMPANY PORTRAIT\n",
            "\n",
            "About LinkedIn: Founded in 2003, LinkedIn connects the world’s professionals to make them more productive and successful. With more than 1 billion members worldwide, including executives from every Fortune 500 company, LinkedIn is the world’s largest professional network. The company has a diversified business model with revenue coming from Talent Solutions, Marketing Solutions, Sales Solutions and Premium Subscriptions products. Headquartered in Silicon Valley, LinkedIn has offices across the globe. Please visit https://www.linkedin.com/company/linkedin/about/ for more information.\n",
            "---\n",
            "## Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering SIGIR ’24, July 14–18, 2024, Washington, DC, USA\n",
            "\n",
            "### PRESENTER BIO\n",
            "\n",
            "Zhentao Xu is a Senior Software Engineer at LinkedIn. He received his M.S. in Robotics and B.S. in Electrical Engineering and Computer Science (EECS) from University of Michigan. His research interests lie in large language models and natural language generation.\n",
            "\n",
            "### REFERENCES\n",
            "\n",
            "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).\n",
            "[2] Atlassian. 2024. Jira | Issue & Project Tracking Software. https://www.atlassian.com/software/jira Accessed: 2024-01-04.\n",
            "[3] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation wip improved correlation wip human judgments. In Proceedings of pe acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. 65–72.\n",
            "[4] Nikita Bhutani, Xinyi Zheng, Kun Qian, Yunyao Li, and H Jagadish. 2020. Answering complex questions by combining information from curated and extracted knowledge bases. In Proceedings of pe first workshop on natural language interfaces. 1–10.\n",
            "[5] Antoine Bordes, Jason Weston, and Nicolas Usunier. 2014. Open question answering wip weakly supervised embedding models. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part I 14. Springer, 165–180.\n",
            "[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\n",
            "[7] Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. 2023. Large Language Models on Graphs: A Comprehensive Survey. arXiv preprint arXiv:2312.02783 (2023).\n",
            "[8] Patrick Lewis, Epan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33 (2020), 9459–9474.\n",
            "[9] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out. 74–81.\n",
            "[10] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023. Reasoning on graphs: Faipful and interpretable large language model reasoning. arXiv preprint arXiv:2310.01061 (2023).\n",
            "\n",
            "[11] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311–318.\n",
            "\n",
            "[12] Qdrant Team. 2024. Qdrant - Vector Database. https://qdrant.tech/. Accessed: 2024-01-08.\n",
            "\n",
            "[13] Zhixiao Qi, Yijiong Yu, Meiqi Tu, Junyi Tan, and Yongfeng Huang. 2023. Foodgpt: A large language model in food testing domain with incremental pre-training and knowledge graph prompt. arXiv preprint arXiv:2308.10173 (2023).\n",
            "\n",
            "[14] Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William Cohen. 2018. Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 4231–4242. https://doi.org/10.18653/v1/D18-1455\n",
            "\n",
            "[15] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo. 2023. Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph. arXiv preprint arXiv:2307.07697 (2023).\n",
            "\n",
            "[16] Christina Unger, Lorenz Bühmann, Jens Lehmann, Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and Philipp Cimiano. 2012. Template-based question answering over RDF data. In Proceedings of the 21st international conference on World Wide Web. 639–648.\n",
            "\n",
            "[17] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533 (2022).\n",
            "\n",
            "[18] Yilin Wen, Zifeng Wang, and Jimeng Sun. 2023. Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models. arXiv preprint arXiv:2308.09729 (2023).\n",
            "\n",
            "[19] Kun Xu, Yansong Feng, Songfang Huang, and Dongyan Zhao. 2016. Hybrid question answering over knowledge base and free text. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. 2397–2407.\n",
            "\n",
            "[20] Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, and Xindong Wu. 2023. ChatGPT is not Enough: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling. arXiv preprint arXiv:2306.11489 (2023).\n",
            "\n",
            "[21] Scott Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. 2015. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Proceedings of the Joint Conference of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing of the AFNLP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 使用GPT Parse Markdown 檔案\n",
        "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
        "\n",
        "node_parser = MarkdownElementNodeParser(\n",
        "    llm=OpenAI(model=LLM_MODEL), num_workers=8\n",
        ")\n",
        "nodes = node_parser.get_nodes_from_documents(md_documents)\n",
        "base_nodes, objects = node_parser.get_nodes_and_objects(nodes)\n",
        "display(Markdown( '\\n\\n'.join([ t.text for t in nodes] )))\n"
      ],
      "metadata": {
        "id": "2LRB_lFUsV5h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "outputId": "df2d091d-fb0d-4ebc-d761-d54f248e6150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4it [00:00, 4778.47it/s]\n",
            "100%|██████████| 4/4 [00:03<00:00,  1.02it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering\n\nZhentao Xu &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Mark Jerome Cruz &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Matthew Guevara\n\nzhexu@linkedin.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; marcruz@linkedin.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mguevara@linkedin.com\n\nLinkedIn Corporation &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; LinkedIn Corporation &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; LinkedIn Corporation\n\nSunnyvale, CA, USA &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Sunnyvale, CA, USA &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Sunnyvale, CA, USA\n\nTie Wang &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Manasi Deshpande &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Xiaofeng Wang\n\ntiewang@linkedin.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; madeshpande@linkedin.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; xiaofwang@linkedin.com\n\nLinkedIn Corporation &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; LinkedIn Corporation &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; LinkedIn Corporation\n\nSunnyvale, CA, USA &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Sunnyvale, CA, USA &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Sunnyvale, CA, USA\n\nZheng Li\n\nzeli@linkedin.com\n\nLinkedIn Corporation\n\nSunnyvale, CA, USA\n\n ABSTRACT\n\nIn customer service technical support, swiftly and accurately retrieving relevant past issues is critical for efficiently resolving customer inquiries. The conventional retrieval methods in retrieval-augmented generation (RAG) for large language models (LLMs) treat a large corpus of past issue tracking tickets as plain text, ignoring the crucial intra-issue structure and inter-issue relations, which limits performance. We introduce a novel customer service question-answering method that amalgamates RAG with a knowledge graph (KG). Our method constructs a KG from historical issues for use in retrieval, retaining the intra-issue structure and inter-issue relations. During the question-answering phase, our method parses consumer queries and retrieves related sub-graphs from the KG to generate answers. This integration of a KG not only improves retrieval accuracy by preserving customer service structure information but also enhances answering quality by mitigating the effects of text segmentation. Empirical assessments on our benchmark datasets, utilizing key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR) metrics, reveal that our method outperforms the baseline by 77.6% in MRR and by 0.32 in BLEU. Our method has been deployed within LinkedIn’s customer service team for approximately six months and has reduced the median per-issue resolution time by 28.6%.\n\n CCS CONCEPTS\n\n• Computing methodologies → Information extraction; Natural language generation.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\nSIGIR ’24, July 14–18, 2024, Washington, DC, USA\n\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n\nCCS CONCEPTS\n\n• Computing methodologies → Information extraction; Natural language generation.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\nSIGIR ’24, July 14–18, 2024, Washington, DC, USA\n\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n\nACM ISBN 979-8-4007-0431-4/24/07\n\nhttps://doi.org/10.1145/3626772.3661370\n\n ACM Reference Format:\n\nZhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, and Zheng Li. 2024. Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’24), July 14–18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 5 pages.\n\nhttps://doi.org/10.1145/3626772.3661370\n\n 1 INTRODUCTION\n\nEffective technical support in customer service underpins product success, directly influencing customer satisfaction and loyalty. Given the frequent similarity of customer inquiries to previously resolved issues, the rapid and accurate retrieval of relevant past instances is crucial for the efficient resolution of such inquiries. Recent advancements in embedding-based retrieval (EBR), large language models (LLMs), and retrieval-augmented generation (RAG) have significantly enhanced retrieval performance and question-answering capabilities for the technical support of customer service. This process typically unfolds in two stages: first, historical issue tickets are treated as plain text, segmented into smaller chunks to accommodate the context length constraints of embedding models; each chunk is then converted into an embedding vector for retrieval. Second, during the question-answering phase, the system retrieves the most relevant chunks and feeds them as contexts for LLMs to generate answers in response to queries. Despite its straightforward approach, this method encounters several limitations:\n\n- Limitation 1 - Compromised Retrieval Accuracy from Ignoring Structures: Issue tracking documents such as Jira possess inherent structure and are interconnected, with references such as \"issue A is related to/copied from/caused\"\n---\n SIGIR ’24, July 14–18, 2024, Washington, DC, USA Zhentao Xu, et al.\n\nLimitation 1 - Loss of Intrinsic Relationship: The conventional approach of compressing documents into text chunks leads to the loss of vital information. Our approach parses issue tickets into trees and further connects individual issue tickets to form an interconnected graph, which maintains this intrinsic relationship among entities, achieving high retrieval performance.\n\nLimitation 2 - Reduced Answer Quality from Segmentation: Segmenting extensive issue tickets into fixed-length segments to accommodate the context length constraints of embedding models can result in the disconnection of related content, leading to incomplete answers. Our graph-based parsing method overcomes this by preserving the logical coherence of ticket sections, ensuring the delivery of complete and high-quality responses.\n\n RELATED WORK\n\nQuestion answering (QA) with knowledge graphs (KGs) can be broadly classified into retrieval-based, template-based, and semantic parsing-based methods. Retrieval-based approaches utilize relation extraction or distributed representations to derive answers from KGs, but they face difficulties with questions involving multiple entities. Template-based strategies depend on manually-created templates for encoding complex queries, yet are limited by the scope of available templates. Semantic parsing methods map text to logical forms containing predicates from KGs.\n\nRecent advancements in large language models (LLMs) integration with Knowledge Graphs (KGs) have demonstrated notable progress. For graph-based reasoning, Think-on-Graph and Reasoning-on-Graph enhance LLMs’ reasoning abilities by integrating KGs. LLM-based question answering systems employ KGs to boost inference capabilities in specialized domains.\n\n METHODS\n\nWe introduce an LLM-based customer service question answering system that seamlessly integrates retrieval-augmented generation (RAG) with a knowledge graph (KG). Our system comprises two phases: KG construction phase and question-answering phase. During the KG construction phase, our system constructs a comprehensive knowledge graph from historical customer service issue tickets. It integrates a tree-structured representation of each issue and interlinks them based on relational context.\n\nSemantic parsing methods map text to logical forms containing predicates from KGs.\n\nRecent advancements in large language models (LLMs) integration with Knowledge Graphs (KGs) have demonstrated notable progress. For graph-based reasoning, Think-on-Graph and Reasoning-on-Graph enhance LLMs’ reasoning abilities by integrating KGs. LLM-based question answering systems employ KGs to boost inference capabilities in specialized domains.\n\n METHODS\n\nWe introduce an LLM-based customer service question answering system that seamlessly integrates retrieval-augmented generation (RAG) with a knowledge graph (KG). Our system comprises two phases: KG construction phase and question-answering phase. During the KG construction phase, our system constructs a comprehensive knowledge graph from historical customer service issue tickets. It integrates a tree-structured representation of each issue and interlinks them based on relational context. It also generates embedding for each node to facilitate later semantic searching. During the question-answering phase, our method parses consumer queries to identify named entities and intents, then navigates within the KG to identify related sub-graphs for generating answers.\n---\n Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering SIGIR ’24, July 14–18, 2024, Washington, DC, USA\n\nThe table provides information on various tickets related to knowledge graph construction, retrieval, and question answering. It includes details on ticket numbers, connections between tickets, entity detection, intent classification, summary of issues, priorities, root causes, and steps to reproduce specific issues.,\nwith the following table title:\nTickets and Connections Summary,\nwith the following columns:\n- Categories: None\n- Details: None\n\n\nThe table provides information on various tickets related to knowledge graph construction, retrieval, and question answering. It includes details on ticket numbers, connections between tickets, entity detection, intent classification, summary of issues, priorities, root causes, and steps to reproduce specific issues.,\nwith the following table title:\nTickets and Connections Summary,\nwith the following columns:\n- Categories: None\n- Details: None\n\n|Knowledge Graph Construction|Retrieval and Question Answering|\n|---|---|\n|Ticket ENT-1744|Ticket ENT-3547|Ticket PORT-133061|CLONE_FROM|Ticket ENT-22970|Question Query: How to reproduce the issue where user saw \"csv upload error in updating user email\" and has major priority that was caused by data issue?|\n|Entity Detection|Intent Classification|\n|2 inter-ticket connection|1 intra-ticket connection|\n|Summary: \"CSV upload error in updating user email\"|Priority: Major|Root Cause: Data Issue|Intent: Steps to Reproduce|\n| |Embedding-based Retrieval|Filtering|Filtering|Question Intent|\n|Inter-ticket connections|Graph Database intra-ticket tree representation|\n|Ticket PORT-133061 \"CSV upload error, updating user email\"|CLONE_FROM|Ticket ENT-22970 \"CSV upload error, updating user email\"|\n|...|...|...|SIMILAR_TO|HAS_FIELDS|HAS_COMMENTS|\n|Ticket ENT-1744|HAS_SUMMARY|HTTP POST csv upload error-internal error|...|\n|Ticket ENT-3547|Learning 'upload csv' option fails|...|\n|Text-embedding Generation for Node Values|Final Answer: based on the ticket ENT-22970, the steps to reproduce the issue is \"1. Refer to the CSV: https://microsoft.sharepoint.com/xxx 2. Open the Dashboard ID xxxxxxxxx 3. Click on Instances > Profile 4. Search for users from the CSV file and note that there are 2 profiles exist.\"|\n\nLegends: issue-tracking ticket, step with step numbers, Vector DB, graph nodes with links, Step with LLM, Graph DB\n\nFigure 1: An overview of our proposed retrieval-augmented generation with knowledge graph framework. The left side of this diagram illustrates the knowledge graph construction; the right side shows the retrieval and question answering process.\n\n 3.2 Retrieval and Question Answering\n\n 3.2.1 Query Entity Identification and Intent Detection.\n\nIn this step, we extract the named entities P of type Map(N → V) and the query intent set I from each user query 𝑞. The method involves parsing each query 𝑞 into a key-value pair, where each key 𝑛, mentioned within the query, corresponds to an element in the graph template T, and the value 𝑣 represents the information extracted from the query. Concurrently, the query intents I include the entities mentioned in the graph template T that the query aims to address. We leverage LLM with a suitable prompt in this parsing process. For instance, given the query 𝑞 = \"How to reproduce the login issue where a user can’t log in to LinkedIn?\", the extracted entity is P = Map(\"issue summary\" → \"login issue\", \"issue description\" → \"user can’t log in to LinkedIn\"), and the intent set is I=Set(\"fix solution\"). This method demonstrates notable flexibility in accommodating varied query formulations by leveraging the LLM’s extensive understanding and interpretive capabilities.\n\n 3.2.2 Embedding-based Retrieval of Sub-graphs.\n\nOur method extracts pertinent sub-graphs from the knowledge graph, aligned with user-provided specifics such as \"issue description\" and \"issue summary\", as well as user intentions like \"fix solution\". This process consists of two primary steps: EBR-based ticket identification and LLM-driven subgraph extraction.\n---\n SIGIR ’24, July 14–18, 2024, Washington, DC, USA\n\nZhentao Xu, et al.\n\nthe same ticket, we rank and select the top 𝐾 ticket tickets. This Table 1: Retrieval Performance method presupposes that the occurrence of multiple query entities is indicative of pertinent links, thus improving retrieval precision.\n\nComparison of Baseline and Experiment results for MRR, Recall@K, and NDCG@K at K=1 and K=3,\nwith the following table title:\nBaseline vs. Experiment Results,\nwith the following columns:\n- MRR: Mean Reciprocal Rank (MRR) values for Baseline and Experiment\n- Recall@K: Recall values at K=1 and K=3 for Baseline and Experiment\n- NDCG@K: Normalized Discounted Cumulative Gain (NDCG) values at K=1 and K=3 for Baseline and Experiment\n\n\nComparison of Baseline and Experiment results for MRR, Recall@K, and NDCG@K at K=1 and K=3,\nwith the following table title:\nBaseline vs. Experiment Results,\nwith the following columns:\n- MRR: Mean Reciprocal Rank (MRR) values for Baseline and Experiment\n- Recall@K: Recall values at K=1 and K=3 for Baseline and Experiment\n- NDCG@K: Normalized Discounted Cumulative Gain (NDCG) values at K=1 and K=3 for Baseline and Experiment\n\n| |MRR|Recall@K|NDCG@K|\n|---|---|---|---|\n|K=1|K=3|K=1|K=3|\n| |Baseline|0.522|0.400|0.640|0.400|0.520|\n| |Experiment|0.927|0.860|1.000|0.860|0.946|\n\nIn the LLM-driven subgraph extraction step, the system first rephrases the original user query 𝑞 to include the retrieved ticket ID; the modified query 𝑞 ′ is then translated into a graph database language, such as Cypher for Neo4j for question answering.\n\nThis table compares the performance metrics (BLEU, METEOR, ROUGE) between a Baseline and an Experiment.,\nwith the following table title:\nPerformance Metrics Comparison,\nwith the following columns:\n- BLEU: None\n- METEOR: None\n- ROUGE: None\n\n\nThis table compares the performance metrics (BLEU, METEOR, ROUGE) between a Baseline and an Experiment.,\nwith the following table title:\nPerformance Metrics Comparison,\nwith the following columns:\n- BLEU: None\n- METEOR: None\n- ROUGE: None\n\n| |BLEU|METEOR|ROUGE|\n|---|---|---|---|\n|Baseline|0.057|0.279|0.183|\n|Experiment|0.377|0.613|0.546|\n\n\nPRODUCTION USE CASE\n\nWe deployed our method within LinkedIn’s customer service team, covering multiple product lines. The team was split randomly into two groups: one used our system, while the other stuck to traditional manual methods. As shown in Table 3, the group using our system achieved significant gains, reducing the median resolution time per issue by 28.6%. This highlights our system’s effectiveness in enhancing customer service efficiency.\n\nThe table compares the time taken for a task when a tool is used versus when the tool is not used.,\nwith the following table title:\nComparison of Task Time with and without Tool,\nwith the following columns:\n- Group: None\n- Mean: None\n- P50: None\n- P90: None\n\n\nThe table compares the time taken for a task when a tool is used versus when the tool is not used.,\nwith the following table title:\nComparison of Task Time with and without Tool,\nwith the following columns:\n- Group: None\n- Mean: None\n- P50: None\n- P90: None\n\n|Group|Mean|P50|P90|\n|---|---|---|---|\n|Tool Not Used|40 Hours|7 Hours|87 Hours|\n|Tool Used|15 hours|5 hours|47 hours|\n\n\nAnswer Generation. Answers are synthesized by correlating retrieved data from Section 3.2.2 with the initial query. The LLM serves as a decoder to formulate responses to user inquiries given the retrieved information. For robust online serving, if query execution encounters issues, a fallback mechanism reverts to a baseline text-based retrieval method.\n\n EXPERIMENT\n\n Experiment Design\n\nOur evaluation employed a curated \"golden\" dataset comprising typical queries, support tickets, and their authoritative solutions. The control group operated with conventional text-based EBR, while the experimental group applied the methodology outlined in this study. For both groups, we utilized the same LLM, specifically GPT-4 [ 1], and the same embedding model, E5 [17 ]. We measured retrieval efficacy using Mean Reciprocal Rank (MRR), recall@K, and NDCG@K. MRR gauges the average inverse rank of the initial correct response, recall@K determines the likelihood of a relevant item’s appearance within the top K selections, and NDCG@K appraises the rank quality by considering both position and pertinence of items. For question-answering performance, we juxtaposed the \"golden\" solutions against the generated responses, utilizing metrics such as BLEU [11], ROUGE [9], and METEOR [3] scores.\n\n Result and Analysis\n\nThe retrieval and question-answering performances are presented in Table 1 and Table 2, respectively. Across all metrics, our method demonstrates consistent improvements. Notably, it surpasses the baseline by 77.6% in MRR and by 0.32 in BLEU score, substantiating its superior retrieval efficacy and question-answering accuracy.\n\n CONCLUSIONS AND FUTURE WORK\n\nIn conclusion, our research significantly advances automated question answering systems for customer service. Integrating retrieval augmented generation (RAG) with a knowledge graph (KG) has improved retrieval and answering metrics, and overall service effectiveness. Future work will focus on: developing an automated mechanism for extracting graph templates, enhancing system adaptability; investigating dynamic updates to the knowledge graph based on user queries to improve real-time responsiveness; and exploring the system’s applicability in other contexts beyond customer service.\n\n COMPANY PORTRAIT\n\nAbout LinkedIn: Founded in 2003, LinkedIn connects the world’s professionals to make them more productive and successful. With more than 1 billion members worldwide, including executives from every Fortune 500 company, LinkedIn is the world’s largest professional network. The company has a diversified business model with revenue coming from Talent Solutions, Marketing Solutions, Sales Solutions and Premium Subscriptions products. Headquartered in Silicon Valley, LinkedIn has offices across the globe. Please visit https://www.linkedin.com/company/linkedin/about/ for more information.\n---\n Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering SIGIR ’24, July 14–18, 2024, Washington, DC, USA\n\n PRESENTER BIO\n\nZhentao Xu is a Senior Software Engineer at LinkedIn. He received his M.S. in Robotics and B.S. in Electrical Engineering and Computer Science (EECS) from University of Michigan. His research interests lie in large language models and natural language generation.\n\n REFERENCES\n\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).\n[2] Atlassian. 2024. Jira | Issue & Project Tracking Software. https://www.atlassian.com/software/jira Accessed: 2024-01-04.\n[3] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation wip improved correlation wip human judgments. In Proceedings of pe acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. 65–72.\n[4] Nikita Bhutani, Xinyi Zheng, Kun Qian, Yunyao Li, and H Jagadish. 2020. Answering complex questions by combining information from curated and extracted knowledge bases. In Proceedings of pe first workshop on natural language interfaces. 1–10.\n[5] Antoine Bordes, Jason Weston, and Nicolas Usunier. 2014. Open question answering wip weakly supervised embedding models. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part I 14.\n\n2005. METEOR: An automatic metric for MT evaluation wip improved correlation wip human judgments. In Proceedings of pe acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. 65–72.\n[4] Nikita Bhutani, Xinyi Zheng, Kun Qian, Yunyao Li, and H Jagadish. 2020. Answering complex questions by combining information from curated and extracted knowledge bases. In Proceedings of pe first workshop on natural language interfaces. 1–10.\n[5] Antoine Bordes, Jason Weston, and Nicolas Usunier. 2014. Open question answering wip weakly supervised embedding models. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part I 14. Springer, 165–180.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\n[7] Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. 2023. Large Language Models on Graphs: A Comprehensive Survey. arXiv preprint arXiv:2312.02783 (2023).\n[8] Patrick Lewis, Epan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33 (2020), 9459–9474.\n[9] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out. 74–81.\n[10] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023. Reasoning on graphs: Faipful and interpretable large language model reasoning. arXiv preprint arXiv:2310.01061 (2023).\n\n[11] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311–318.\n\n[12] Qdrant Team. 2024. Qdrant - Vector Database. https://qdrant.tech/. Accessed: 2024-01-08.\n\n[13] Zhixiao Qi, Yijiong Yu, Meiqi Tu, Junyi Tan, and Yongfeng Huang. 2023. Foodgpt: A large language model in food testing domain with incremental pre-training and knowledge graph prompt. arXiv preprint arXiv:2308.10173 (2023).\n\n[14] Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and William Cohen. 2018. Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 4231–4242. https://doi.org/10.18653/v1/D18-1455\n\n[15] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo. 2023. Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph. arXiv preprint arXiv:2307.07697 (2023).\n\n[16] Christina Unger, Lorenz Bühmann, Jens Lehmann, Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and Philipp Cimiano. 2012. Template-based question answering over RDF data. In Proceedings of the 21st international conference on World Wide Web. 639–648.\n\nAssociation for Computational Linguistics, Brussels, Belgium, 4231–4242. https://doi.org/10.18653/v1/D18-1455\n\n[15] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo. 2023. Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph. arXiv preprint arXiv:2307.07697 (2023).\n\n[16] Christina Unger, Lorenz Bühmann, Jens Lehmann, Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and Philipp Cimiano. 2012. Template-based question answering over RDF data. In Proceedings of the 21st international conference on World Wide Web. 639–648.\n\n[17] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533 (2022).\n\n[18] Yilin Wen, Zifeng Wang, and Jimeng Sun. 2023. Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models. arXiv preprint arXiv:2308.09729 (2023).\n\n[19] Kun Xu, Yansong Feng, Songfang Huang, and Dongyan Zhao. 2016. Hybrid question answering over knowledge base and free text. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. 2397–2407.\n\n[20] Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, and Xindong Wu. 2023. ChatGPT is not Enough: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling. arXiv preprint arXiv:2306.11489 (2023).\n\n[21] Scott Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. 2015. Semantic parsing via staged query graph generation: Question answering with knowledge base. In Proceedings of the Joint Conference of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing of the AFNLP."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 使用ChatGPT 翻譯\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    # Defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "TRANSLATE_MODEL = \"gpt-3.5-turbo-0125\"  # @param {type:\"string\"}\n",
        "SYSTEM_PROMPT = '請你成為文章翻譯的小幫手，請協助翻譯以下技術文件，以繁體中文輸出'  # @param {type:\"string\"}\n",
        "def translate_text(text):\n",
        "  completion = client.chat.completions.create(\n",
        "    model=TRANSLATE_MODEL,\n",
        "    messages=[\n",
        "          {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "          {\"role\": \"user\", \"content\":text},\n",
        "      ]\n",
        "  )\n",
        "  return completion.choices[0].message.content\n",
        "\n",
        "translated_text = []\n",
        "for node in nodes:\n",
        "  translated_text.append(translate_text(node.text))\n",
        "\n",
        "\n",
        "display(Markdown( '\\n\\n'.join([ t for t in translated_text] )))\n"
      ],
      "metadata": {
        "id": "wuMiLfpSs-zw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "outputId": "2b8dca0f-ba13-4033-a13f-40113af14b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "知識圖譜檢索增強生成用於客戶服務問答\n\n許振濤 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Mark Jerome Cruz &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Matthew Guevara\n\nzhexu@linkedin.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; marcruz@linkedin.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; mguevara@linkedin.com\n\nLinkedIn Corporation &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; LinkedIn Corporation &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; LinkedIn Corporation\n\n美國加州聖尼維爾\n\nTie Wang &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Manasi Deshpande &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Xiaofeng Wang\n\ntiewang@linkedin.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; madeshpande@linkedin.com &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; xiaofwang@linkedin.com\n\nLinkedIn Corporation &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; LinkedIn Corporation &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; LinkedIn Corporation\n\n美國加州聖尼維爾\n\nZheng Li\n\nzeli@linkedin.com\n\nLinkedIn Corporation\n\n美國加州聖尼維爾\n\n摘要\n\n在客戶服務技術支援中，迅速准確地檢索相關的過往問題對於高效解決客戶查詢至關重要。傳統的檢索增強生成（RAG）方法對於大型語言模型（LLMs）中的大型問題追蹤票據將其作為純文本處理，忽略了關鍵的內部問題結構和問題之間的關係，這限制了性能。我們引入了一種新的客戶服務問答方法，將RAG與知識圖譜（KG）相結合。我們的方法從歷史問題構建知識圖譜以用於檢索，保留了內部問題結構和問題之間的關係。在問答階段，我們的方法解析消費者查詢並從知識圖譜中檢索相關子圖以生成答案。通過將知識圖譜整合到KG中，不僅通過保留客服結構信息提高了檢索的準確性，還通過減輕文本切割的影響來提高了答案的質量。通過在我們的基準數據集上進行實證評估，利用關鍵檢索（MRR、Recall@K、NDCG@K）和文本生成（BLEU、ROUGE、METEOR）指標，結果顯示我們的方法在MRR方面超越基線77.6%，在BLEU方面超越基線0.32。我們的方法已在LinkedIn的客戶服務團隊中使用約六個月，將每個問題的解決中位數時間降低了28.6%。\n\nCCS概念\n\n• 計算方法論 → 信息提取；自然語言生成。\n\nCCS概念\n\n• 計算方法論 → 信息提取；自然語言生成。\n\n未經許可，任何個人或教室均可製作此作品的數碼或印刷拷貝，前提是不得因此牟利或商業優勢，拷貝必須註明此通知和第一頁的完整引用。此作品中其他作者擁有版權的部分，必須予以尊重。允許帶有信用摘要。未經授權或付費，不得複製、再出版、張貼在伺服器上或重新分發到清單中。請向 permissions@acm.org 請求許可。\n\nSIGIR ’24，2024年7月14日至18日，美國華盛頓特區\n\n© 2024 版權歸擁有者/作者持有。發表權利許可給ACM。\n\nACM ISBN 979-8-4007-0431-4/24/07\n\nhttps://doi.org/10.1145/3626772.3661370\n\n ACM 參考格式：\n\nZhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang 和 Zheng Li. 2024. 使用知識圖和檢索增強的生成進行客戶服務問答. 在第47屆國際ACM SIGIR信息檢索研究與開發大會 (SIGIR ’24) 會議論文集中，2024年7月14日至18日，美國華盛頓特區。ACM，美國紐約，5頁。\n\nhttps://doi.org/10.1145/3626772.3661370\n\n 1 引言\n\n客戶服務中有效的技術支援是產品成功的基礎，直接影響客戶滿意度和忠誠度。考慮到客戶詢問通常與先前解決的問題相似，迅速且準確地檢索相關的過去實例對於高效解決此類問題至關重要。最近在基於嵌入的檢索（EBR）、大型語言模型（LLMs）和檢索增強的生成（RAG）方面的進展顯著提升了技術支援客戶服務的檢索性能和問答能力。這一過程通常分為兩個階段：首先，將歷史問題工單視為純文本，分段為較小的塊以容納嵌入模型的上下文長度約束；然後將每個塊轉換為一個嵌入向量以進行檢索。其次，在問答階段，系統檢索最相關的塊並將它們作為上下文提供給LLMs，以根據查詢產生答案。儘管這種方法看似簡單，但遇到了幾個限制：\n\n- 限制1 - 從忽略結構中犧牲檢索準確性：問題追踪文檔如Jira具有固有結構且相互關聯，包括“問題A與...有關/復制自/導致”\n\n---\n SIGIR ’24，2024年7月14日至18日，美國華盛頓特區，Zhentao Xu等。\n\n限制1 - 因分割而失去內在關係：將文檔壓縮為文本塊的傳統方法會導致重要信息的丟失。我們的方法將問題工單解析為樹狀結構，並進一步連接單個問題工單以形成一個相互關聯的圖，保持實體之間的內在關係，從而實現高效的檢索性能。\n\n限制2 - 因分割導致答案質量降低：將廣泛的問題工單分割為固定長度的段落以滿足嵌入模型的上下文長度約束可能導致相關內容的分離，導致答案不完整。我們基於圖的解析方法通過保留票務部分的邏輯一致性來克服這一問題，確保提供完整和高質量的答案。\n\n 相關工作\n\n基於知識圖（KGs）的問答（QA）可以廣泛分為檢索式、基於模板和基於語義解析的方法。檢索式方法利用關係提取或分佈式表示來從KGs中產生答案，但在涉及多個實體的問題上存在困難。基於模板的策略依賴於手動創建的模板來編碼複雜查詢，但受到可用模板範圍的限制。語義解析方法將文本映射到包含來自KGs的謂詞的邏輯形式。\n\n最近在大型語言模型（LLMs）與知識圖（KGs）集成方面取得了顯著進展。針對圖形式的推理，Think-on-Graph和Reasoning-on-Graph通過整合KGs增強了LLMs的推理能力。基於LLMs的問答系統利用KGs提升了專業領域中的推理能力。\n\n 方法\n\n我們介紹了一個基於LLM的客戶服務問答系統，該系統無縫集成了檢索增強的生成（RAG）和知識圖（KG）。我們的系統包括兩個階段：知識圖構建階段和問答階段。在知識圖構建階段，我們的系統從歷史客戶服務問題工單中構建了一個全面的知識圖。它使用每個問題的樹狀結構化表示，並根據關聯上下文將它們進行互連。\n\n語義分析方法將文本對應到包含知識圖譜中謂詞的邏輯形式。\n\n最近將大型語言模型（LLMs）與知識圖譜（KGs）相結合的先進發展展示了顯著進步。對於基於圖譜的推理，「Think-on-Graph」和「Reasoning-on-Graph」通過整合知識圖譜來增強LLMs的推理能力。基於LLMs的問答系統利用知識圖譜來提升在專門領域中的推理能力。\n\n方法\n\n我們介紹一個基於LLMs的客戶服務問答系統，該系統通過無縫集成檢索增強生成（RAG）和知識圖譜（KG）。我們的系統由兩個階段組成：知識圖譜構建階段和問答階段。在知識圖譜構建階段，我們的系統從歷史客戶服務問題工單中構建一個全面的知識圖譜。它基於關係上下文將每個問題構建為樹狀結構表示，並互相鏈接。它還為每個節點生成嵌入以方便後續的語義搜索。在問答階段，我們的方法解析消費者的查詢以識別命名實體和意圖，然後在知識圖譜中尋找相關子圖用於生成答案。\n\n檢索增強生成結合知識圖譜的客戶服務問答 SIGIR '24，2024年7月14-18日，美國華盛頓特區。\n\n該表格提供了與知識圖譜構建、檢索和問答相關的各種票證信息。它包括票證編號、票證之間的連接、實體檢測、意圖分類、問題總結、優先級、根本原因以及再現特定問題的步驟。\n\n表格標題如下：\n票證與連接摘要\n\n表格包含以下列：\n- 類別：無\n- 詳細信息：無\n\n知識圖譜建構、檢索和問答相關的各種問題的票證信息如下表所示。它包括有關票證編號、票證之間的關聯、實體檢測、意圖分類、問題摘要、優先順序、根本原因以及重現特定問題的步驟的詳細資訊。,\n\n以下是表的標題:\n票證與關聯摘要\n以下是表的欄位:\n- 分類：無\n- 詳細資料：無\n\n|知識圖譜建構|檢索和問答|\n|---|---|\n|票證 ENT-1744|票證 ENT-3547|票證 PORT-133061|從 CLONE_FROM|票證 ENT-22970|問題查詢：如何重現使用者看到的「在更新使用者電子郵件時發生csv上傳錯誤」問題，並且有重要優先順序，這是由數據問題引起的？|\n|實體檢測|意圖分類|\n|2 個票證之間的關聯|1 個票證內的關聯|\n|摘要：「在更新使用者電子郵件時發生CSV上傳錯誤」|優先順序：重要|根本原因：數據問題|意圖：重現步驟|\n| |基於嵌入的檢索|過濾|過濾|問題意圖|\n|票證 PORT-133061「CSV上傳錯誤，更新使用者電子郵件」|從 CLONE_FROM|票證 ENT-22970「CSV上傳錯誤，更新使用者電子郵件」|\n|...|...|...|相似於|有欄位|有評論|\n|票證 ENT-1744|有摘要|HTTP POST csv上傳錯誤-內部錯誤|...|\n|票證 ENT-3547|學習'上傳csv'選項失敗|...|\n|用於節點值的文本嵌入生成|最終答案：根據票證 ENT-22970，重現問題的步驟為「1. 參考CSV：https://microsoft.sharepoint.com/xxx 2. 打開儀表板ID xxxxxxxxx 3. 點擊“實例” > “檔案” 4. 從CSV文檔中搜索用戶並注意到有 2 個檔案存在。」|\n\n傳說：問題追踪工單、有步驟號碼的步驟、向量資料庫、具有連結的圖形節點、帶有LLM的步驟、圖形資料庫\n\n圖1：我們提出的知識圖形框架檢索增強生成的概述。該圖的左側顯示知識圖形構建，右側顯示檢索和問答過程。\n\n3.2 檢索和問答\n\n3.2.1 查詢實體識別和意圖檢測。\n\n在這一步驟中，我們從每個用戶查詢𝑞中提取類型為Map(N → V)的命名實體P和查詢意圖集I。該方法涉及將每個查詢𝑞解析為鍵值對，其中在查詢中提到的每個鍵𝑛對應於圖形模板T中的一個元素，值𝑣代表從查詢中提取的信息。與此同時，查詢意圖I包括在圖形模板T中提到的查詢旨在解決的實體。我們在這一解析過程中利用LLM和合適的提示。例如，對於查詢𝑞 = \"如何重現用戶無法登錄到LinkedIn的登錄問題？\"，提取的實體為P = Map(\"問題摘要\" → \"登錄問題\"， \"問題描述\" → \"用戶無法登錄到LinkedIn\" )，意圖集合為I=Set(\"修復解決方案\")。該方法展示了通過利用LLM廣泛的理解和解釋能力，適應各種查詢形式的顯著靈活性。\n\n3.2.2 基於嵌入式檢索的子圖檢索。\n\n我們的方法從知識圖中提取與用戶提供的特定信息，如 \"問題描述\" 和 \"問題摘要\"，以及用戶意圖如 \"修復解決方案\" 相一致的相關子圖。該過程包括兩個主要步驟：基於嵌入式檢索的工單識別和由LLM驅動的子圖提取。\n\n基準和實驗結果在K=1和K=3時的MRR、Recall@K和NDCG@K的比較，\n包含以下表格標題：\n基準與實驗結果比較，\n包含以下欄位：\n- MRR：基準和實驗的平均倒數排名（MRR）值\n- Recall@K：在K=1和K=3時基準和實驗的召回率值\n- NDCG@K：在K=1和K=3時基準和實驗的歸一化折扣積分增益（NDCG）值\n\n基準結果和實驗結果的MRR、Recall@K和NDCG@K在K=1和K=3的比較如下表所示：\n表格標題：基準 vs. 實驗結果\n\n|    |MRR    |Recall@K    |NDCG@K    |\n|---|---|---|---|\n|K=1    |0.522    |0.640    |0.520    |\n|K=3    |0.400    |0.400    |NA    |\n|    |Baseline    |0.927    |1.000    |0.946    |\n|    |Experiment    |0.860    |0.860    |NA    |\n\n在由LLM驅動的子圖提取步驟中，系統首先將原始使用者查詢𝑞重新表述，以包含檢索到的票證ID；然後將修改後的查詢𝑞'翻譯為圖形數據庫語言，例如Cypher用於Neo4j進行問答。\n\n這個表格比較了基準版和實驗版之間的性能指標（BLEU、METEOR、ROUGE），\n表格標題為：\n性能指標比較，\n包括以下列：\n- BLEU：無\n- METEOR：無\n- ROUGE：無\n\n這張表格比較了基準組與實驗組之間的表現指標（BLEU、METEOR、ROUGE），\n表格標題為：表現指標比較，\n包含以下列：\n- BLEU: 無\n- METEOR: 無\n- ROUGE: 無\n\n| |BLEU|METEOR|ROUGE|\n|---|---|---|---|\n|基準組|0.057|0.279|0.183|\n|實驗組|0.377|0.613|0.546|\n\n生產使用案例\n\n我們在 LinkedIn 的客戶服務團隊中部署了我們的方法，涵蓋多個產品線。團隊被隨機分成兩組：一組使用我們的系統，另一組堅持使用傳統的手動方法。如第3表所示，使用我們系統的團隊取得了顯著的收益，將每個問題的中位解決時間減少了28.6%。這突顯了我們系統在提高客戶服務效率方面的有效性。\n\n該表格比較了使用工具和不使用工具時完成任務所需的時間，\n與以下表格標題：\n使用與不使用工具的任務時間比較，\n具有以下列：\n- 組別：無\n- 平均值：無\n- P50：無\n- P90：無\n\n以下是比較使用工具和不使用工具完成任務所需時間的表格:\n\n| 群組 | 平均值 | P50 | P90 |\n|---|---|---|---|\n| 未使用工具 | 40小時 | 7小時 | 87小時 |\n| 使用工具 | 15小時 | 5小時 | 47小時 |\n\n答案生成。答案是通過將從第3.2.2節檢索的數據與初始查詢相關聯來綜合合成的。LLM作為解碼器，根據獲取的信息來制定對用戶查詢的回答。對於強大的在線服務，如果查詢執行遇到問題，回退機制將恢復到基礎的基於文本的檢索方法。\n\n實驗\n\n實驗設計\n\n我們的評估采用了一個由典型查詢、支持工單及其權威解決方案組成的精選“黃金”數據集。對照組使用傳統的基於文本的EBR，而實驗組應用了本研究中概述的方法。對於兩組，我們使用了相同的LLM，具體是GPT-4 [1]，和相同的嵌入模型，即E5 [17]。我們使用平均倒數排名（MRR）、召回率@K和NDCG@K來衡量檢索效果。MRR評估初始正確回答的平均倒數排名，召回率@K確定相關項目在前K個選擇中出現的可能性，NDCG@K通過同時考慮項目的位置和相關性來評估排名質量。對於問答性能，我們將“黃金”解決方案與生成的回答進行對比，使用BLEU [11]、ROUGE [9]和METEOR [3]等指標進行評分。\n\n結果和分析\n\n檢索和問答表現請見表1和表2。在所有指標上，我們的方法展示了一致的改進。值得注意的是，它在MRR方面超越了基準77.6％，在BLEU評分方面超過了0.32，證實了其優越的檢索效能和問答準確性。\n\n結論和未來工作\n\n總之，我們的研究顯著推進了客戶服務的自動問答系統。將檢索增強生成（RAG）與知識圖（KG）相結合已提升了檢索和回答指標，提高了整體服務效果。未來的工作將專注於：開發一個自動機制來提取圖形模板，增強系統的適應性；根據用戶查詢研究基於知識圖的動態更新，以改善實時響應能力；探索系統在客戶服務之外其他情境中的應用性。\n\n公司簡介\n\n關於LinkedIn: 成立於2003年，LinkedIn連接全球專業人士，使他們更具生產力和成功。擁有超過10億全球會員，包括來自每一家財富500強企業的高管在內，LinkedIn是全球最大的專業網絡。該公司擁有多樣化的業務模式，收入來自人才解決方案、營銷解決方案、銷售解決方案和高級訂閱產品。總部位於矽谷，LinkedIn在全球各地設有辦事處。欲了解更多信息，請訪問https://www.linkedin.com/company/linkedin/about/。\n\n演示人簡介\n\n徐振濤是LinkedIn的高級軟體工程師。他在密歇根大學獲得機器人學碩士學位和電機工程與計算機科學（EECS）學士學位。他的研究興趣包括大型語言模型和自然語言生成。\n\n參考文獻\n\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, 等人。2023。GPT-4技術報告。arXiv預印本arXiv:2303.08774 (2023)。\n[2] Atlassian。2024。Jira | 項目跟踪軟體。https://www.atlassian.com/software/jira。訪問日期：2024年01月04日。\n[3] Satanjeev Banerjee和Alon Lavie。2005。METEOR:一種用於機器翻譯評估的自動度量標準，並提高與人類判斷之間的相關性。在pe acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization的論文集中。65-72頁。\n[4] Nikita Bhutani, Xinyi Zheng, Kun Qian, Yunyao Li和H Jagadish。2020。通過結合來自精心策劃和提取的知識庫信息來回答複雜問題。在第一屆自然語言接口研討會的論文集中。1-10頁。\n[5] Antoine Bordes, Jason Weston和Nicolas Usunier。2014。使用弱監督嵌入模型進行開放問答。在歐洲會議ECML PKDD 2014的機器學習和知識發現中：2014年9月15-19日，法國南錫。部分I 14。\n\n2005年。METEOR：一種自動評估機器翻譯的度量標準，具有較高的與人類判斷相關性。在ACL研討會工作坊 \"intrisic and extrinsic evaluation measures for machine translation and/or summarization\" 的論文集上。65-72頁。\n\n[4] Nikita Bhutani, Xinyi Zheng, Kun Qian, Yunyao Li 和 H Jagadish. 2020. 通過結合從編纂和提取的知識庫信息來回答複雜問題。收錄於第一屆自然語言界面研討會論文集。1-10頁。\n\n[5] Antoine Bordes, Jason Weston和Nicolas Usunier。2014. 使用弱監督嵌入模型的開放式問答。收錄於《機器學習和知識發現在數據庫中的應用：歐洲會議ECML PKDD 2014論文集，第I部分》。Springer，165-180頁。\n\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee和Kristina Toutanova。2018. BERT：深度雙向Transformer的語言理解預訓練。arXiv preprint arXiv:1810.04805 (2018)。\n\n[7] Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji和Jiawei Han。2023。圖上的大型語言模型：一項全面調查。arXiv preprint arXiv:2312.02783 (2023)。\n\n[8] Patrick Lewis, Epan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel等。2020。知識密集型NLP任務的檢索增強生成。《神經信息處理系統進展》第33卷 (2020)，9459-9474頁。\n\n[9] Chin-Yew Lin。2004。Rouge：自動摘要評估的一個套件。在文本摘要分支中。74-81頁。\n\n[10] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari和Shirui Pan。2023。圖上的推理：Faipful和可解釋的大型語言模型推理。arXiv preprint arXiv:2310.01061 (2023)。\n\n[11] Kishore Papineni, Salim Roukos, Todd Ward和Wei-Jing Zhu。2002。Bleu：自動機器翻譯評估方法。在“計算語言學年會第40屆年會論文集”。311-318頁。\n\n[12] Qdrant團隊。2024。Qdrant - 向量數據庫。https://qdrant.tech/。檢索日期：2024年1月8日。\n\n[13] Zhixiao Qi, Yijiong Yu, Meiqi Tu, Junyi Tan和Yongfeng Huang。2023。FoodGPT：在食品測試領域的大型語言模型，具有增量式預訓練和知識圖提示。arXiv preprint arXiv:2308.10173 (2023)。\n\n[14] Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov和William Cohen。2018。使用知識庫和文本的早期融合進行開放域問答。《2018年自然語言處理實證方法會議論文集》，由Ellen Riloff, David Chiang, Julia Hockenmaier和Jun’ichi Tsujii（編輯）。計算語言學協會，比利時布魯塞爾，4231-4242頁。https://doi.org/10.18653/v1/D18-1455。\n\n[15] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum和Jian Guo。2023。圖上的思考：具有知識圖的大型語言模型的深度和負責任推理。arXiv preprint arXiv:2307.07697 (2023)。\n\n[16] Christina Unger, Lorenz Bühmann, Jens Lehmann, Axel-Cyrille Ngonga Ngomo, Daniel Gerber和Philipp Cimiano。2012。基於模板的RDF數據上的問答。在“第21屆國際萬維網大會論文集”。639-648頁。\n\n計算語言學協會，比利時布魯塞爾，4231–4242。https://doi.org/10.18653/v1/D18-1455\n\n[15] 孫嘉碩，徐成勁，唐露明，王賽卓，林晨，龔夜云，沈向陽，郭劍。2023年。Think-on-graph：知識圖譜與大型語言模型的深度和負責任推理。arXiv預印本arXiv:2307.07697（2023）。\n\n[16] 克里斯蒂娜·烏格（Christina Unger），羅倫茨·布曼（Lorenz Bühmann），廖鷲（Jens Lehmann），阿克塞爾-西里爾·恩戈納·恩哥莫（Axel-Cyrille Ngonga Ngomo），丹尼爾·格爾伯（Daniel Gerber）和菲利普·西米亞諾（Philipp Cimiano）。2012年。基於模板的RDF數據問答。在第21屆世界網際網路大會論文集中。639–648。\n\n[17] 王亮，楊楠，黃曉龍，焦彬星，楊琳軍，江達新，拉根·馬朱姆德（Rangan Majumder）和韋富茹（Furu Wei）。2022年。弱監督對比預訓練下的文本嵌入。arXiv預印本arXiv:2212.03533（2022）。\n\n[18] 聞毅麟，王自封，孫際猛。2023年。思維導圖：知識圖譜促發大型語言模型中的思維圖。arXiv預印本arXiv:2308.09729（2023）。\n\n[19] 徐堃，馮言松，黃松方，趙東岩。2016年。基於知識庫和自由文本的混合問答。在第26屆計算語言學國際會議（COLING 2016）論文集中。2397–2407。\n\n[20] 楊林曜，陳洪洋，李釗，丁霄，吳新東。2023年。ChatGPT不足以應對：利用知識圖譜增強大型語言模型進行事實感知語言建模。arXiv預印本arXiv:2306.11489（2023）。\n\n[21] 易文討（Scott Wen-tau Yih），張明偉，賀曉東，高建鋒。2015年。通過分段查詢圖生成進行語義解析：與知識庫的問答。在ACL第53屆年會和AFNLP第7屆國際聯合會議的程序中。"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 儲存翻譯結果\n",
        "\n",
        "import os\n",
        "DONE_LOC = os.path.join(THESIS_LOC, 'done')\n",
        "\n",
        "if not os.path.exists(DONE_LOC):\n",
        "  os.makedirs(DONE_LOC)\n",
        "\n",
        "with open(os.path.join(THESIS_LOC, PDF_FILE + '.md'), 'w') as f:\n",
        "  f.write('\\n\\n'.join([ t for t in translated_text] ))\n",
        "\n",
        "os.rename(os.path.join(THESIS_LOC, PDF_FILE), os.path.join(DONE_LOC, PDF_FILE) )\n",
        "\n",
        "os.listdir(THESIS_LOC)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "1fC46HeE0j8r",
        "outputId": "639deb76-06db-42c7-e8d4-0a7b6178c1a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['done', '2404.17723.pdf.md']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lgu540bf93XT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}